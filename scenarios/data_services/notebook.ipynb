{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6eb106b7",
   "metadata": {},
   "source": [
    "# Data Services for demo run\n",
    "See more examples of using data services on the link: https://github.com/th2-net/th2-data-services-utils/tree/master/examples/notebooks\n",
    "\n",
    "Data services core lib: https://github.com/th2-net/th2-data-services"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e51640a",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "----------------------------------------------\n",
    "\n",
    "Jupyter notebook for demo run analysis.\n",
    "\n",
    "## [1] General statistic\n",
    "\n",
    "### [1.1] Number of Test Events within requested time range\n",
    "\n",
    "### [1.2] Statistics by event type and execution status\n",
    "The table aggregates all event types and shows how many of them and how many of them were failed.\n",
    "\n",
    "\n",
    "\n",
    "## [2] Recons analysis\n",
    "\n",
    "### [2.1] Summary table  \n",
    "The table summarizes the work of the check2-recon th2 component, including the list of executed rules and their statuses.\n",
    "\n",
    "### [2.2] Distribution by message type\n",
    "This table presents how many messages were processed per each message type.\n",
    "\n",
    "\n",
    "\n",
    "## [3] Script analysis\n",
    "\n",
    "### [3.1] Basic statistics by test cases\n",
    "Shows how many test cases were failed and passed.\n",
    "\n",
    "### [3.2] Detailed test case statistics\n",
    "This table shows all test runs, their test cases, statuses and time execution.\n",
    "\n",
    "### [3.3] Failed Verifications\n",
    "The table shows detailed information on failed verifications, including test event ids, failed tags and test case details. \n",
    "\n",
    "### [3.4] Plot all data aggregated by supertype into a single chart with filters\n",
    "\n",
    "----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c1f9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from IPython.core.display import display, HTML\n",
    "from datetime import datetime, timedelta\n",
    "from th2_data_services.events_tree import EventsTree\n",
    "from th2_data_services.provider.v5.data_source.http import HTTPProvider5DataSource\n",
    "from th2_data_services.provider.v5.commands import http\n",
    "from th2_data_services.filter import Filter\n",
    "from th2_data_services.provider.v5.events_tree import (\n",
    "    EventsTreeCollectionProvider5, \n",
    "    ParentEventsTreeCollectionProvider5\n",
    ")\n",
    "from th2_data_services.data import Data\n",
    "from th2_data_services_utils import utils as Utils\n",
    "from pandas import DataFrame, Grouper\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import yaml\n",
    "\n",
    "# This settings for increase display jupyter notebook and dataframe table.\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "pd.options.display.max_rows = 1500\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "\n",
    "# For understand which event type on based name we get from stream.\n",
    "def get_super_type(record):\n",
    "    name = record.get(\"eventName\")\n",
    "    parent_id = record.get(\"parentEventId\")\n",
    "    super_type = record.get(\"eventType\")\n",
    "    if super_type == \"\":\n",
    "        if \"Recon\" in name:\n",
    "            super_type = \"Recon Folder\"\n",
    "        else:\n",
    "            if not parent_id:\n",
    "                super_type = \"Test Run\"\n",
    "            else:\n",
    "                parent_event = collection.get_event(parent_id)\n",
    "                if parent_event:\n",
    "                    parent_super_type = get_super_type(parent_event)\n",
    "                    if parent_super_type == \"Test Run\":\n",
    "                        super_type = \"Test Case\"\n",
    "                    elif parent_super_type == \"Recon Folder\":\n",
    "                        super_type = \"Recon Rule\"\n",
    "                    elif parent_super_type == \"Recon Rule\":\n",
    "                        super_type = \"Recon Status\"\n",
    "                    elif parent_super_type == \"Recon Status\":\n",
    "                        super_type = \"Recon Event\"\n",
    "\n",
    "    return super_type\n",
    "\n",
    "# Base extract (transform function)\n",
    "# record is required arguments.\n",
    "def extract_basic(record):\n",
    "    new_object = {}\n",
    "    start_time = datetime.fromtimestamp(record.get(\"startTimestamp\", {}).get(\"epochSecond\", 0))\n",
    "    start_time += timedelta(microseconds=record.get(\"startTimestamp\", {}).get(\"nano\", 0)/1000)\n",
    "    end_time = datetime.fromtimestamp(record.get(\"endTimestamp\", {}).get(\"epochSecond\", 0))\n",
    "    end_time += timedelta(microseconds=record.get(\"endTimestamp\", {}).get(\"nano\", 0)/1000)\n",
    "    new_object.update(\n",
    "        {\n",
    "            \"super_type\": get_super_type(record),\n",
    "            \"start_time\": start_time,\n",
    "            \"end_time\": end_time,\n",
    "            \"status\": \"SUCCESSFUL\" if record.get(\"successful\") else \"FAILED\",\n",
    "            \"eventName\": record.get(\"eventName\"),\n",
    "            \"eventId\": record.get(\"eventId\"),\n",
    "            \"parentEventId\": record.get(\"parentEventId\"),\n",
    "            \"body\": record.get(\"body\"),\n",
    "            \"attachedMessageIds\": record.get(\"attachedMessageIds\")\n",
    "        }\n",
    "    )\n",
    "    if new_object['eventName'] is None:\n",
    "        pprint(record)\n",
    "    return new_object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cfe3d0",
   "metadata": {},
   "source": [
    "## Start and finish test time setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35612fda",
   "metadata": {},
   "source": [
    "Get start and finish test time automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5831ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open('start_datetime.pickle', 'rb') as f:\n",
    "    START_TIME = pickle.load(f)\n",
    "with open('finish_datetime.pickle', 'rb') as f:\n",
    "    END_TIME = pickle.load(f)\n",
    "#START_TIME = datetime(2022, 3, 30, 15, 58, 40, 174859)\n",
    "#END_TIME = datetime(2022, 3, 30, 16, 3, 18, 56148)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85bbca8",
   "metadata": {},
   "source": [
    "Get DataSource URL from config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b1e7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../configs/ds_config.yaml') as f:\n",
    "    data = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    URL = data['provider_data_source']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd899927",
   "metadata": {},
   "source": [
    "Create DataSource and EventsTree objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9511636",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_source = HTTPProvider5DataSource(URL)\n",
    "events: Data = data_source.command(\n",
    "    http.GetEvents(\n",
    "        start_timestamp=START_TIME,\n",
    "        end_timestamp=END_TIME,\n",
    "        attached_messages=True,\n",
    "        cache=True,\n",
    "    )\n",
    ")\n",
    "    \n",
    "collection = EventsTreeCollectionProvider5(events, data_source=data_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137b4b2f",
   "metadata": {},
   "source": [
    "# [1] General statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8b6141",
   "metadata": {},
   "source": [
    "## [1.1] Number of Events in the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5171318a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3086ede9",
   "metadata": {},
   "source": [
    "## [1.2] Statistic by all event types\n",
    "The table aggregates all event types and shows how many of them and how many of them were failed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acddf600",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Utils.aggregate_by_groups(events.map(extract_basic), \"super_type\", total_row=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01320c49",
   "metadata": {},
   "source": [
    "# [2] Recons analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3827dc2",
   "metadata": {},
   "source": [
    "## [2.1] Summary table\n",
    "The table summarizes the work of the check2-recon th2 component, including the list of executed rules and their statuses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a349c274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_output(record):\n",
    "    full_path = collection.get_full_path(record.get(\"eventId\"))\n",
    "    new_obj = {\n",
    "        \"Recon Root\": full_path[0].get(\"eventName\"),\n",
    "        \"Recon Rule\" : full_path[1].get(\"eventName\"),\n",
    "        \"Rule Status\": full_path[2].get(\"eventName\"),\n",
    "        \"Number of Events\": 1,\n",
    "    }\n",
    "    return new_obj\n",
    "\n",
    "data: Data = events\\\n",
    "        .map(extract_basic)\\\n",
    "        .filter(lambda record: record.get(\"super_type\") == \"Recon Event\")\\\n",
    "        .map(transform_output)\\\n",
    "\n",
    "# Functions from pandas.\n",
    "df = DataFrame(data).groupby(['Recon Root', \"Recon Rule\", 'Rule Status']).agg({\"Number of Events\": \"sum\"})\n",
    "df = Utils.append_total_rows(df, {\"Number of Events\": \"sum\"})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da350cec",
   "metadata": {},
   "source": [
    "## [2.2] Distribution by message type\n",
    "This table presents how many messages were processed per each message type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6124374e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_recon_ancestor(record):\n",
    "    parent_id = record.get(\"parentEventId\")\n",
    "    if parent_id is not None:\n",
    "        ancestor = collection.find_ancestor(record.get(\"eventId\"), lambda record: get_super_type(record) == \"Recon Folder\")\n",
    "        if ancestor:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_match_or_match_failed(record):\n",
    "    ancestor = collection.find_ancestor(record.get(\"eventId\"), lambda record: record.get(\"eventName\") == \"No match\")\n",
    "    if ancestor and not ancestor.get(\"successful\"):\n",
    "        return True\n",
    "    ancestor = collection.find_ancestor(record.get(\"eventId\"), lambda record: record.get(\"eventName\") == \"Matched passed\")\n",
    "    if ancestor:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def exctract_block(record):\n",
    "    messages_id = record.get(\"attachedMessageIds\")\n",
    "    \n",
    "    if not messages_id:\n",
    "        return None\n",
    "    \n",
    "    messages = data_source.command(http.GetMessagesById(messages_id))\n",
    "    output = [{\"MsgType\": message.get(\"body\", {}).get(\"metadata\", {}).get(\"messageType\")} for message in messages]\n",
    "    ## [2.2] Distribution by message type\n",
    "    ## This table presents how many messages were processed per each message type.\n",
    "    return output\n",
    "\n",
    "data = events\\\n",
    "        .map(extract_basic)\\\n",
    "        .filter(is_recon_ancestor)\\\n",
    "        .filter(is_match_or_match_failed)\\\n",
    "        .map(exctract_block)\n",
    "\n",
    "df = DataFrame(data)\n",
    "df.groupby(\"MsgType\").size().reset_index(name=\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4abb31d",
   "metadata": {},
   "source": [
    "# [3] Script analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a95bc8",
   "metadata": {},
   "source": [
    "## [3.1] Basic statistics by test cases\n",
    "Shows how many test cases were failed and passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6517fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_output(record):    \n",
    "    new_obj = {\n",
    "        \"Test Case\": 1,\n",
    "        \"Status\": record.get(\"status\")\n",
    "    }\n",
    "    return new_obj\n",
    "\n",
    "data = events\\\n",
    "        .map(extract_basic)\\\n",
    "        .filter(lambda record: record.get(\"super_type\") == \"Test Case\")\\\n",
    "        .map(transform_output)\n",
    "\n",
    "df = DataFrame(data=data)\n",
    "df = df.groupby([\"Status\"]).sum()\n",
    "df[\"Percent\"] = df[\"Test Case\"] / df[\"Test Case\"].sum() * 100\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e184b1a7",
   "metadata": {},
   "source": [
    "## [3.2] Detailed test case statistics\n",
    "This table shows all test runs, their test cases, statuses and time execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48b2101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ancestor_is_test_case(record):\n",
    "    if not record.get(\"parentEventId\"):\n",
    "        return False\n",
    "    ancestor = collection.find_ancestor(record.get(\"eventId\"), lambda record: get_super_type(record) == \"Test Case\")\n",
    "    if ancestor:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def transform_output(record):\n",
    "    fullpath = collection.get_full_path(record.get(\"eventId\"))\n",
    "    \n",
    "    start_time = datetime.fromtimestamp(fullpath[1].get(\"startTimestamp\", {}).get(\"epochSecond\", 0))\n",
    "    start_time += timedelta(microseconds=fullpath[1].get(\"startTimestamp\", {}).get(\"nano\", 0)/1000)\n",
    "    \n",
    "    message_id = record.get(\"attachedMessageIds\")\n",
    "    if not message_id:\n",
    "        return None\n",
    "    message_id = message_id[0]\n",
    "    \n",
    "    message = data_source.command(http.GetMessageById(message_id))\n",
    "    if not message:\n",
    "        return None\n",
    "    \n",
    "    body = message.get(\"body\", {})\n",
    "    if not body:\n",
    "        return None\n",
    "    \n",
    "    end_time = body.get(\"metadata\", {}).get(\"timestamp\")\n",
    "    end_time = datetime.strptime(end_time, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "    end_time += timedelta(hours=3)\n",
    "    \n",
    "    new_obj = {\n",
    "        \"Test Run\": fullpath[0].get(\"eventName\"),\n",
    "        \"Test Case\": fullpath[1].get(\"eventName\"),\n",
    "        \"Status\": \"SUCCESSFUL\" if fullpath[1].get(\"successful\") else \"FAILED\",\n",
    "        'Start Time': start_time,\n",
    "        'End Time': end_time,\n",
    "    }\n",
    "    return new_obj\n",
    "\n",
    "data = events\\\n",
    "        .map(extract_basic)\\\n",
    "        .filter(ancestor_is_test_case)\\\n",
    "        .filter(lambda record: record.get(\"super_type\") in [\"Verification\", \"message\"])\\\n",
    "        .map(transform_output)\n",
    "\n",
    "df = DataFrame(data=data)\n",
    "df = df.groupby([\"Test Run\", \"Test Case\", \"Status\"]).agg({\"Start Time\": \"min\", \"End Time\": \"max\"}).reset_index()\n",
    "df[\"duration\"] = df[\"End Time\"] - df[\"Start Time\"]\n",
    "df.sort_values(by=[\"Start Time\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30610bc3",
   "metadata": {},
   "source": [
    "## [3.3] Failed Verifications\n",
    "The table shows detailed information on failed verifications, including test event ids, failed tags and test case details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f12eb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def is_test_case_ancestor(record):\n",
    "    parent_id = record.get(\"parentEventId\")\n",
    "    if parent_id is not None:\n",
    "        ancestor = collection.find_ancestor(record.get(\"eventId\"), lambda record: get_super_type(record) == \"Test Case\")\n",
    "        if ancestor:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def extract_failed_tags(record):\n",
    "    new_obj = {\n",
    "        \"Event Id\": record.get(\"eventId\"),\n",
    "        \"Event Name\": record.get(\"eventName\"),\n",
    "        \"Test Run\": collection.find_ancestor(record.get(\"eventId\"), lambda record: get_super_type(record) == \"Test Run\").get(\"eventName\"),\n",
    "        \"Test Case\": collection.find_ancestor(record.get(\"eventId\"), lambda record: get_super_type(record) == \"Test Case\").get(\"eventName\"),\n",
    "    }\n",
    "    for content in record.get(\"body\"):\n",
    "        tags = Utils.search_fields(content, \"OrderCapacity\", \"AccountType\")\n",
    "    new_obj.update({\"tags\": tags})\n",
    "    return new_obj\n",
    "\n",
    "data = events\\\n",
    "        .map(extract_basic)\\\n",
    "        .filter(is_test_case_ancestor)\\\n",
    "        .filter(lambda record: record.get(\"super_type\") == \"Verification\")\\\n",
    "        .filter(lambda record: record.get(\"status\") == \"FAILED\")\\\n",
    "        .map(extract_failed_tags)\n",
    "\n",
    "transform_data = []\n",
    "for i in data:\n",
    "    common = {\n",
    "        \"Event Name\": i.get(\"Event Name\"),\n",
    "        \"Test Run\": i.get(\"Test Run\"),\n",
    "        \"Test Case\": i.get(\"Test Case\"),\n",
    "        \"Event_Id\": i.get(\"Event Id\"),\n",
    "    }\n",
    "    for tag, payload in i[\"tags\"].items():\n",
    "        for value in payload:\n",
    "            if value.get(\"status\") == \"FAILED\":\n",
    "                transform_data.append({\n",
    "                    **common, \n",
    "                    \"failed_tag\": tag, \n",
    "                    \"failed_actual\": value.get(\"actual\"),\n",
    "                    \"failed_expected\": value.get(\"expected\"),\n",
    "                    \"failed_operation\": value.get(\"operation\")\n",
    "                })\n",
    "\n",
    "# From pandas for comforted view\n",
    "failed_verifications = DataFrame(data=transform_data)\n",
    "failed_verifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755ca88a",
   "metadata": {},
   "source": [
    "## [3.4] Plot all data aggregated by supertype into a single chart with filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547714c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_output(record):\n",
    "    new_obj = {\n",
    "        \"time\": record.get(\"start_time\"),\n",
    "        \"super_type\": record.get(\"super_type\"),\n",
    "        \"status\": record.get(\"status\"),\n",
    "    }\n",
    "    return new_obj\n",
    "\n",
    "data = events\\\n",
    "         .map(extract_basic)\\\n",
    "         .filter(lambda record: record.get(\"super_type\") in [\"Recon Event\", \"Verification\"])\\\n",
    "         .map(transform_output)\n",
    "\n",
    "df = Utils.aggregate_groups_by_intervals(data, \"time\", \"super_type\", intervals=\"10s\", pivot=\"super_type\")\n",
    "Utils.create_tick_diagram(df)  # The plot may not be shown if you have not restarted the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c9f7c6",
   "metadata": {},
   "source": [
    "## [3.5] Latency density\n",
    "Searches pairs messages with type NewOrderSingle and ExecutionReport. Then calculates latency and demonstrates on plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fb3be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_new_single_order_or_execution_report(record):\n",
    "    body = record.get(\"body\")\n",
    "    if body:\n",
    "        message_type = body.get(\"metadata\", {}).get(\"messageType\")\n",
    "        if message_type in [\"NewOrderSingle\", \"ExecutionReport\"]:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def clear_unnecessery_fields(record):\n",
    "    new_obj = None\n",
    "    body = record.get(\"body\")\n",
    "    if body:\n",
    "        fields = body.get(\"fields\", {})\n",
    "        clOrdID = fields.get(\"ClOrdID\", {}).get(\"simpleValue\")\n",
    "        ord_status = fields.get(\"OrdStatus\", {}).get(\"simpleValue\")\n",
    "        \n",
    "        metadata = body.get(\"metadata\", {})\n",
    "        message_type = metadata.get(\"messageType\")\n",
    "        session_alias = metadata.get(\"id\", {}).get(\"connectionId\", {}).get(\"sessionAlias\")\n",
    "        time = metadata.get(\"timestamp\")\n",
    "        \n",
    "        new_obj = {\n",
    "            \"clOrdID\": clOrdID,\n",
    "            \"OrdStatus\": ord_status,\n",
    "            \"MessageType\": message_type,\n",
    "            \"sessionAlias\": session_alias,\n",
    "            \"time\": time,\n",
    "        }\n",
    "    return new_obj\n",
    "\n",
    "streams = set()\n",
    "for record in collection.get_all_events_iter():\n",
    "    messages = record.get(\"attachedMessageIds\")\n",
    "    for msg in messages:\n",
    "        streams.add(msg.split(\":\")[0])\n",
    "        \n",
    "messages = data_source.command(\n",
    "    http.GetMessages(\n",
    "        start_timestamp=START_TIME,\n",
    "        end_timestamp=END_TIME,\n",
    "        stream=list(streams)\n",
    "    )\n",
    ")\n",
    "\n",
    "data = messages\\\n",
    "        .filter(is_new_single_order_or_execution_report)\\\n",
    "        .map(clear_unnecessery_fields)\n",
    "\n",
    "roundtrips = {}\n",
    "latency = []\n",
    "\n",
    "for record in data:\n",
    "    msg_type = record.get(\"MessageType\")\n",
    "    clOrdID = record.get(\"clOrdID\")\n",
    "    \n",
    "    if msg_type == \"NewOrderSingle\":\n",
    "        if clOrdID not in roundtrips:\n",
    "            roundtrips[clOrdID] = record.get(\"time\")\n",
    "    elif msg_type == \"ExecutionReport\":\n",
    "        if record.get(\"OrdStatus\") == '0':\n",
    "            if clOrdID in roundtrips:\n",
    "                current_latency = datetime.strptime(record.get(\"time\"), \"%Y-%m-%dT%H:%M:%S.%fZ\") -  datetime.strptime(roundtrips[clOrdID], \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "                latency.append({\"latency\": 1, \"time\": datetime.strptime(str(current_latency), \"%H:%M:%S.%f\")})\n",
    "\n",
    "df = DataFrame(data=latency).set_index(\"time\").groupby(Grouper(freq=\"10ms\")).sum()\n",
    "df.index = df.index.strftime(\"%S.%f\")\n",
    "\n",
    "Utils.create_tick_diagram(df)  # The plot may not be shown if you have not restarted the notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
